{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4j5zXeyjW6o0dfeHA9ug0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshuaDayalGnanamuthu/ModularNeuralNet/blob/main/ModularNeuralNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(Z):\n",
        "    \"\"\"Sigmoid activation function\"\"\"\n",
        "    return 1 / (1 + np.exp(-np.clip(Z, -500, 500)))\n",
        "\n",
        "def sigmoid_derivative(Z):\n",
        "    \"\"\"Derivative of sigmoid function\"\"\"\n",
        "    s = sigmoid(Z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def relu(Z):\n",
        "    \"\"\"ReLU activation function\"\"\"\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "def relu_derivative(Z):\n",
        "    \"\"\"Derivative of ReLU function\"\"\"\n",
        "    return np.where(Z > 0, 1, 0)\n",
        "\n",
        "def tanh(Z):\n",
        "    \"\"\"Hyperbolic tangent activation function\"\"\"\n",
        "    return np.tanh(Z)\n",
        "\n",
        "def tanh_derivative(Z):\n",
        "    \"\"\"Derivative of tanh function\"\"\"\n",
        "    return 1 - np.power(np.tanh(Z), 2)\n",
        "\n",
        "activations = {\n",
        "    'sigmoid': (sigmoid, sigmoid_derivative),\n",
        "    'relu': (relu, relu_derivative),\n",
        "    'tanh': (tanh, tanh_derivative)\n",
        "}\n",
        "\n",
        "class ModularNeuralNet:\n",
        "    def __init__(self, input_size, hidden_layers=None, activation='sigmoid', final_activation='sigmoid'):\n",
        "        \"\"\"\n",
        "        Initialize a neural network with customizable architecture.\n",
        "\n",
        "        Parameters:\n",
        "        - input_size: Number of input features\n",
        "        - hidden_layers: List of integers specifying the size of each hidden layer\n",
        "        - activation: Activation function to use ('sigmoid', 'relu', or 'tanh')\n",
        "        - final_activation: Activation function for output layer\n",
        "        \"\"\"\n",
        "        self.input_size = input_size\n",
        "\n",
        "        if not hidden_layers:\n",
        "            hidden_layers = [8, 4, 1]\n",
        "        elif not isinstance(hidden_layers[-1], int) or hidden_layers[-1] != 1:\n",
        "            hidden_layers[-1] = 1\n",
        "\n",
        "        self.layers = [input_size] + hidden_layers\n",
        "        self.num_layers = len(self.layers)\n",
        "\n",
        "        if activation in activations:\n",
        "            self.activation, self.activation_derivative = activations[activation]\n",
        "        else:\n",
        "            raise ValueError(f\"Activation function '{activation}' is not supported. Use 'sigmoid', 'relu', or 'tanh'.\")\n",
        "\n",
        "        if final_activation in activations:\n",
        "            self.final_activation, self.final_activation_derivative = activations[final_activation]\n",
        "        else:\n",
        "            raise ValueError(f\"Final activation function '{final_activation}' is not supported.\")\n",
        "\n",
        "        self.parameters = self.initialize_parameters()\n",
        "        self.best_parameters = None\n",
        "\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        \"\"\"Initialize parameters (Weights/Biases) with He initialization\"\"\"\n",
        "        parameters = {}\n",
        "        for i in range(1, self.num_layers):\n",
        "            if self.activation == relu:\n",
        "                factor = np.sqrt(2/self.layers[i-1])\n",
        "            else:\n",
        "                factor = np.sqrt(1/self.layers[i-1])\n",
        "            parameters[f'W{i}'] = np.random.randn(self.layers[i], self.layers[i-1]) * factor\n",
        "            parameters[f'b{i}'] = np.zeros((self.layers[i], 1))\n",
        "        return parameters\n",
        "\n",
        "\n",
        "    def feed_forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward propagation through the network.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input data of shape (n_features, m_examples) where m is number of examples\n",
        "\n",
        "        Returns:\n",
        "        - AL: Output of the last layer\n",
        "        - caches: Dictionary containing activations and pre-activations for backprop\n",
        "        \"\"\"\n",
        "        caches = {'A0': X}\n",
        "        A = X\n",
        "\n",
        "        for i in range(1, self.num_layers):\n",
        "            previous = A\n",
        "            W = self.parameters[f'W{i}']\n",
        "            b = self.parameters[f'b{i}']\n",
        "            Z = W @ previous + b\n",
        "\n",
        "            if i == self.num_layers - 1:\n",
        "                A = self.final_activation(Z)\n",
        "            else:\n",
        "                A = self.activation(Z)\n",
        "\n",
        "            caches[f'A{i}'] = A\n",
        "            caches[f'Z{i}'] = Z\n",
        "        return A, caches\n",
        "\n",
        "    def cost(self, A, Y):\n",
        "        \"\"\"\n",
        "        Compute binary cross-entropy cost.\n",
        "\n",
        "        Parameters:\n",
        "        - A: Output of the last layer, shape (1, m)\n",
        "        - Y: True labels, shape (1, m)\n",
        "\n",
        "        Returns:\n",
        "        - cost: Binary cross-entropy cost\n",
        "        \"\"\"\n",
        "        m = Y.shape[1]\n",
        "        epsilon = 1e-15\n",
        "        A = np.clip(A, epsilon, 1 - epsilon)\n",
        "        cost = -1/m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
        "        return np.squeeze(cost)\n",
        "\n",
        "    def backward_propagation(self, Y, caches):\n",
        "        \"\"\"\n",
        "        Backward propagation to compute gradients.\n",
        "\n",
        "        Parameters:\n",
        "        - Y: True labels, shape (1, m)\n",
        "        - caches: Dictionary containing values from forward prop\n",
        "\n",
        "        Returns:\n",
        "        - gradients: Dictionary containing gradients\n",
        "        \"\"\"\n",
        "        m = Y.shape[1]\n",
        "        gradients = {}\n",
        "\n",
        "        for i in range(self.num_layers - 1, 0, -1):\n",
        "            A = caches[f'A{i}']\n",
        "            previous = caches[f'A{i-1}']\n",
        "            Z = caches[f'Z{i}']\n",
        "\n",
        "            if i == (self.num_layers - 1):\n",
        "                dA = -(np.divide(Y, A) - np.divide(1 - Y, 1 - A))\n",
        "                dZ = dA * self.final_activation_derivative(Z)\n",
        "            else:\n",
        "                dZ = dA * self.activation_derivative(Z)\n",
        "\n",
        "            gradients[f'dW{i}'] = 1/m * (dZ @ previous.T)\n",
        "            gradients[f'db{i}'] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
        "\n",
        "            if i > 1:\n",
        "                W = self.parameters[f'W{i}']\n",
        "                dA = W.T @ dZ\n",
        "\n",
        "        return gradients\n",
        "\n",
        "    def refresh_paramters(self, gradients, learning_rate):\n",
        "        \"\"\"\n",
        "        Update parameters using gradient descent.\n",
        "\n",
        "        Parameters:\n",
        "        - gradients: Dictionary containing gradients\n",
        "        - learning_rate: Learning rate for gradient descent\n",
        "        \"\"\"\n",
        "        for i in range(1, self.num_layers):\n",
        "            self.parameters[f'W{i}'] -= learning_rate * gradients[f'dW{i}']\n",
        "            self.parameters[f'b{i}'] -= learning_rate * gradients[f'db{i}']\n",
        "\n",
        "\n",
        "    def train(self, X, Y, epochs=1000, learning_rate=0.01, batch_size=None,\n",
        "              print_interval=100, learning_rate_decay=0.95, decay_interval=100,\n",
        "              early_stopping_patience=None, validation_data=None):\n",
        "        \"\"\"\n",
        "        Train the neural network.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input data of shape (n_examples, n_features)\n",
        "        - Y: True labels of shape (n_examples,) or (n_examples, 1)\n",
        "        - epochs: Number of training epochs\n",
        "        - learning_rate: Initial learning rate\n",
        "        - batch_size: Size of mini-batches (if None, use all data)\n",
        "        - print_interval: How often to print progress\n",
        "        - learning_rate_decay: Factor to decay learning rate\n",
        "        - decay_interval: How often to decay learning rate\n",
        "        - early_stopping_patience: Stop if no improvement after this many epochs\n",
        "        - validation_data: Tuple of (X_val, Y_val) for validation\n",
        "\n",
        "        Returns:\n",
        "        - history: Dictionary of training metrics\n",
        "        \"\"\"\n",
        "        X = np.array(X)\n",
        "        Y = np.array(Y)\n",
        "        if len(Y.shape) == 1:\n",
        "            Y = Y.reshape(-1, 1)\n",
        "\n",
        "        X_train = X.T\n",
        "        Y_train = Y.T\n",
        "\n",
        "        history = {\n",
        "            'costs': [],\n",
        "            'val_costs': [] if validation_data else None,\n",
        "            'learning_rates': []\n",
        "        }\n",
        "\n",
        "        best_val_cost = float('inf')\n",
        "        patience_counter = 0\n",
        "        current_learning_rate = learning_rate\n",
        "        m = X_train.shape[1]\n",
        "\n",
        "        if validation_data:\n",
        "            X_val, Y_val = validation_data\n",
        "            if len(Y_val.shape) == 1:\n",
        "                Y_val = Y_val.reshape(-1, 1)\n",
        "            X_val = X_val.T\n",
        "            Y_val = Y_val.T\n",
        "\n",
        "        for i in range(epochs):\n",
        "            if batch_size is None:\n",
        "                A, caches = self.feed_forward(X_train)\n",
        "                cost = self.cost(A, Y_train)\n",
        "                gradients = self.backward_propagation(Y_train, caches)\n",
        "                self.refresh_paramters(gradients, current_learning_rate)\n",
        "            else:\n",
        "                cost = 0\n",
        "                permutation = np.random.permutation(m)\n",
        "                X_shuffled = X_train[:, permutation]\n",
        "                Y_shuffled = Y_train[:, permutation]\n",
        "\n",
        "                num_batches = int(np.ceil(m / batch_size))\n",
        "\n",
        "                for j in range(num_batches):\n",
        "                    start_idx = j * batch_size\n",
        "                    end_idx = min((j + 1) * batch_size, m)\n",
        "\n",
        "                    X_batch = X_shuffled[:, start_idx:end_idx]\n",
        "                    Y_batch = Y_shuffled[:, start_idx:end_idx]\n",
        "\n",
        "                    A_batch, caches_batch = self.feed_forward(X_batch)\n",
        "                    batch_cost = self.cost(A_batch, Y_batch)\n",
        "                    cost += batch_cost * (end_idx - start_idx) / m\n",
        "\n",
        "                    gradients = self.backward_propagation(Y_batch, caches_batch)\n",
        "                    self.refresh_parameters(gradients, current_learning_rate)\n",
        "\n",
        "            history['costs'].append(cost)\n",
        "            history['learning_rates'].append(current_learning_rate)\n",
        "\n",
        "            if validation_data:\n",
        "                A_val, _ = self.feed_forward(X_val)\n",
        "                val_cost = self.cost(A_val, Y_val)\n",
        "                history['val_costs'].append(val_cost)\n",
        "\n",
        "                if early_stopping_patience:\n",
        "                    if val_cost < best_val_cost:\n",
        "                        best_val_cost = val_cost\n",
        "                        patience_counter = 0\n",
        "                        self.best_parameters = {k: v.copy() for k, v in self.parameters.items()}\n",
        "                    else:\n",
        "                        patience_counter += 1\n",
        "                        if patience_counter > early_stopping_patience:\n",
        "                            print(f\"Early stopping at epoch {i}, no improvement for {early_stopping_patience} epochs\")\n",
        "                            if self.best_parameters:\n",
        "                                self.parameters = self.best_parameters\n",
        "                            break\n",
        "\n",
        "            elif early_stopping_patience:\n",
        "                if cost < best_val_cost:\n",
        "                    best_val_cost = cost\n",
        "                    patience_counter = 0\n",
        "                    self.best_parameters = {k: v.copy() for k, v in self.parameters.items()}\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if patience_counter > early_stopping_patience:\n",
        "                        print(f\"Early stopping at epoch {i}, no improvement for {early_stopping_patience} epochs\")\n",
        "                        if self.best_parameters:\n",
        "                            self.parameters = self.best_parameters\n",
        "                        break\n",
        "\n",
        "            if i > 0 and i % decay_interval == 0:\n",
        "                current_learning_rate *= learning_rate_decay\n",
        "\n",
        "            if i % print_interval == 0 or i == epochs - 1:\n",
        "                status = f\"Epoch {i}: cost = {cost:.6f}, learning_rate = {current_learning_rate:.6f}\"\n",
        "                if validation_data:\n",
        "                    status += f\", val_cost = {val_cost:.6f}\"\n",
        "                print(status)\n",
        "\n",
        "        if early_stopping_patience and validation_data and self.best_parameters:\n",
        "            self.parameters = self.best_parameters\n",
        "\n",
        "        return history\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        \"\"\"\n",
        "        Make predictions using the trained model.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input data of shape (n_examples, n_features)\n",
        "        - threshold: Classification threshold for binary predictions\n",
        "\n",
        "        Returns:\n",
        "        - predictions: Binary predictions (0 or 1)\n",
        "        \"\"\"\n",
        "        X = np.array(X)\n",
        "        if X.shape[0] != self.input_size and X.shape[1] == self.input_size:\n",
        "            X = X.T\n",
        "\n",
        "        A, _ = self.feed_forward(X)\n",
        "        predictions = (A > threshold).astype(int)\n",
        "\n",
        "        return predictions.T\n",
        "\n",
        "    def predict_probability(self, X):\n",
        "        \"\"\"\n",
        "        Get probability estimates (useful for ROC curves, etc.)\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input data of shape (n_examples, n_features)\n",
        "\n",
        "        Returns:\n",
        "        - probabilities: Probability estimates (0-1)\n",
        "        \"\"\"\n",
        "        X = np.array(X)\n",
        "        if X.shape[0] != self.input_size and X.shape[1] == self.input_size:\n",
        "            X = X.T\n",
        "\n",
        "        A, _ = self.feed_forward(X)\n",
        "        return A.T\n",
        "\n",
        "    def evaluate(self, X, Y):\n",
        "        \"\"\"\n",
        "        Evaluate model performance.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input data of shape (n_examples, n_features)\n",
        "        - Y: True labels of shape (n_examples,) or (n_examples, 1)\n",
        "\n",
        "        Returns:\n",
        "        - metrics: Dictionary with performance metrics\n",
        "        \"\"\"\n",
        "        X = np.array(X)\n",
        "        Y = np.array(Y)\n",
        "\n",
        "        if len(Y.shape) == 1:\n",
        "            Y = Y.reshape(-1, 1)\n",
        "\n",
        "        predictions = self.predict(X)\n",
        "        probabilities = self.predict_probability(X)\n",
        "        accuracy = np.mean(predictions == Y)\n",
        "\n",
        "        true_positives = np.sum((predictions == 1) & (Y == 1))\n",
        "        false_positives = np.sum((predictions == 1) & (Y == 0))\n",
        "        false_negatives = np.sum((predictions == 0) & (Y == 1))\n",
        "\n",
        "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        loss = self.cost(probabilities.T, Y.T)\n",
        "\n",
        "        metrics = {\n",
        "            'accuracy': accuracy * 100,\n",
        "            'precision': precision * 100,\n",
        "            'recall': recall * 100,\n",
        "            'f1_score': f1 * 100,\n",
        "            'loss': loss\n",
        "        }\n",
        "\n",
        "        return metrics, predictions\n",
        "\n",
        "    def save_model(self, filename):\n",
        "        \"\"\"Save model parameters to a file\"\"\"\n",
        "        model_data = {\n",
        "            'input_size': self.input_size,\n",
        "            'layers': self.layers,\n",
        "            'parameters': {k: v.tolist() for k, v in self.parameters.items()},\n",
        "            'activation': next(name for name, (func, _) in activations.items()\n",
        "                              if func == self.activation),\n",
        "            'final_activation': next(name for name, (func, _) in activations.items()\n",
        "                                    if func == self.final_activation)\n",
        "        }\n",
        "        np.save(filename, model_data, allow_pickle=True)\n",
        "        print(f\"Model saved to {filename}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, filename):\n",
        "        \"\"\"Load model from a file\"\"\"\n",
        "        model_data = np.load(filename, allow_pickle=True).item()\n",
        "        model = cls(\n",
        "            input_size=model_data['input_size'],\n",
        "            hidden_layers=model_data['layers'][1:],\n",
        "            activation=model_data['activation'],\n",
        "            final_activation=model_data['final_activation']\n",
        "        )\n",
        "        for key, value in model_data['parameters'].items():\n",
        "            model.parameters[key] = np.array(value)\n",
        "\n",
        "        return model"
      ],
      "metadata": {
        "id": "YK-aL-toJFpJ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "path = kagglehub.dataset_download(\"johnsmith88/heart-disease-dataset\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "\n",
        "df = pd.read_csv(path + \"/heart.csv\")\n",
        "print(df.head())\n",
        "\n",
        "X = df.drop(\"target\", axis=1)\n",
        "y = df[\"target\"]\n",
        "\n",
        "categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
        "numeric_cols = [col for col in X.columns if col not in categorical_cols]\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numeric_cols),\n",
        "    ('cat', OneHotEncoder(drop=\"first\"), categorical_cols)\n",
        "])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "\n",
        "print(\"Processed train shape:\", X_train_processed.shape)\n",
        "print(\"Processed test shape:\", X_test_processed.shape)\n",
        "\n",
        "model = ModularNeuralNet(\n",
        "        input_size=22,\n",
        "        hidden_layers=[128, 64, 16, 1],\n",
        "        activation=\"relu\"\n",
        "    )\n",
        "\n",
        "\n",
        "model.train(\n",
        "    X_train_processed,\n",
        "    y_train,\n",
        "    epochs=5000,\n",
        "    learning_rate=0.01,\n",
        "    print_interval=100,\n",
        "    learning_rate_decay=0.9,\n",
        "    decay_interval=500,\n",
        "    early_stopping_patience=300\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSOof9yYJHzv",
        "outputId": "6a9d6425-a76a-475e-df58-bee6ef0de9a3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/heart-disease-dataset\n",
            "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
            "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
            "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
            "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
            "3   61    1   0       148   203    0        1      161      0      0.0      2   \n",
            "4   62    0   0       138   294    1        1      106      0      1.9      1   \n",
            "\n",
            "   ca  thal  target  \n",
            "0   2     3       0  \n",
            "1   0     3       0  \n",
            "2   0     3       0  \n",
            "3   1     3       0  \n",
            "4   3     2       0  \n",
            "Processed train shape: (820, 22)\n",
            "Processed test shape: (205, 22)\n",
            "Epoch 0: cost = 0.906160, learning_rate = 0.010000\n",
            "Epoch 100: cost = 0.418249, learning_rate = 0.010000\n",
            "Epoch 200: cost = 0.362618, learning_rate = 0.010000\n",
            "Epoch 300: cost = 0.327647, learning_rate = 0.010000\n",
            "Epoch 400: cost = 0.299231, learning_rate = 0.010000\n",
            "Epoch 500: cost = 0.274489, learning_rate = 0.009000\n",
            "Epoch 600: cost = 0.254874, learning_rate = 0.009000\n",
            "Epoch 700: cost = 0.237621, learning_rate = 0.009000\n",
            "Epoch 800: cost = 0.222591, learning_rate = 0.009000\n",
            "Epoch 900: cost = 0.209104, learning_rate = 0.009000\n",
            "Epoch 1000: cost = 0.196967, learning_rate = 0.008100\n",
            "Epoch 1100: cost = 0.186973, learning_rate = 0.008100\n",
            "Epoch 1200: cost = 0.177579, learning_rate = 0.008100\n",
            "Epoch 1300: cost = 0.168780, learning_rate = 0.008100\n",
            "Epoch 1400: cost = 0.160578, learning_rate = 0.008100\n",
            "Epoch 1500: cost = 0.152908, learning_rate = 0.007290\n",
            "Epoch 1600: cost = 0.146290, learning_rate = 0.007290\n",
            "Epoch 1700: cost = 0.140037, learning_rate = 0.007290\n",
            "Epoch 1800: cost = 0.133667, learning_rate = 0.007290\n",
            "Epoch 1900: cost = 0.127374, learning_rate = 0.007290\n",
            "Epoch 2000: cost = 0.121244, learning_rate = 0.006561\n",
            "Epoch 2100: cost = 0.115789, learning_rate = 0.006561\n",
            "Epoch 2200: cost = 0.110559, learning_rate = 0.006561\n",
            "Epoch 2300: cost = 0.105588, learning_rate = 0.006561\n",
            "Epoch 2400: cost = 0.100870, learning_rate = 0.006561\n",
            "Epoch 2500: cost = 0.096389, learning_rate = 0.005905\n",
            "Epoch 2600: cost = 0.092504, learning_rate = 0.005905\n",
            "Epoch 2700: cost = 0.088741, learning_rate = 0.005905\n",
            "Epoch 2800: cost = 0.085114, learning_rate = 0.005905\n",
            "Epoch 2900: cost = 0.081619, learning_rate = 0.005905\n",
            "Epoch 3000: cost = 0.078255, learning_rate = 0.005314\n",
            "Epoch 3100: cost = 0.075331, learning_rate = 0.005314\n",
            "Epoch 3200: cost = 0.072495, learning_rate = 0.005314\n",
            "Epoch 3300: cost = 0.069743, learning_rate = 0.005314\n",
            "Epoch 3400: cost = 0.067036, learning_rate = 0.005314\n",
            "Epoch 3500: cost = 0.064434, learning_rate = 0.004783\n",
            "Epoch 3600: cost = 0.062152, learning_rate = 0.004783\n",
            "Epoch 3700: cost = 0.059956, learning_rate = 0.004783\n",
            "Epoch 3800: cost = 0.057836, learning_rate = 0.004783\n",
            "Epoch 3900: cost = 0.055778, learning_rate = 0.004783\n",
            "Epoch 4000: cost = 0.053799, learning_rate = 0.004305\n",
            "Epoch 4100: cost = 0.052084, learning_rate = 0.004305\n",
            "Epoch 4200: cost = 0.050436, learning_rate = 0.004305\n",
            "Epoch 4300: cost = 0.048841, learning_rate = 0.004305\n",
            "Epoch 4400: cost = 0.047297, learning_rate = 0.004305\n",
            "Epoch 4500: cost = 0.045804, learning_rate = 0.003874\n",
            "Epoch 4600: cost = 0.044508, learning_rate = 0.003874\n",
            "Epoch 4700: cost = 0.043258, learning_rate = 0.003874\n",
            "Epoch 4800: cost = 0.042053, learning_rate = 0.003874\n",
            "Epoch 4900: cost = 0.040889, learning_rate = 0.003874\n",
            "Epoch 4999: cost = 0.039773, learning_rate = 0.003874\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'costs': [np.float64(0.9061597816932926),\n",
              "  np.float64(0.8734135675786516),\n",
              "  np.float64(0.8442134207142746),\n",
              "  np.float64(0.8180770362132209),\n",
              "  np.float64(0.7951662006966042),\n",
              "  np.float64(0.7740980874306951),\n",
              "  np.float64(0.7547594735309443),\n",
              "  np.float64(0.7369730263143371),\n",
              "  np.float64(0.7206319472117776),\n",
              "  np.float64(0.7058831483644963),\n",
              "  np.float64(0.6922488305294673),\n",
              "  np.float64(0.6797128290912247),\n",
              "  np.float64(0.6683094260653507),\n",
              "  np.float64(0.6577917980242566),\n",
              "  np.float64(0.6480472595712393),\n",
              "  np.float64(0.6390286516179183),\n",
              "  np.float64(0.630676454711513),\n",
              "  np.float64(0.6227630719566384),\n",
              "  np.float64(0.6151450413836381),\n",
              "  np.float64(0.6079662262866435),\n",
              "  np.float64(0.6010829607639506),\n",
              "  np.float64(0.5945451056321438),\n",
              "  np.float64(0.5883068081430508),\n",
              "  np.float64(0.5823594435670345),\n",
              "  np.float64(0.5766600038479993),\n",
              "  np.float64(0.5711997687024586),\n",
              "  np.float64(0.5660133903550048),\n",
              "  np.float64(0.5610889919265989),\n",
              "  np.float64(0.5563761090836588),\n",
              "  np.float64(0.5518106395350477),\n",
              "  np.float64(0.5474132520065308),\n",
              "  np.float64(0.543188560611445),\n",
              "  np.float64(0.539122118270481),\n",
              "  np.float64(0.5352050360032826),\n",
              "  np.float64(0.5314303468721728),\n",
              "  np.float64(0.5277807849050672),\n",
              "  np.float64(0.5242787961898618),\n",
              "  np.float64(0.5208801348091089),\n",
              "  np.float64(0.5176018698649762),\n",
              "  np.float64(0.514452631599935),\n",
              "  np.float64(0.511410457873144),\n",
              "  np.float64(0.5084504656764494),\n",
              "  np.float64(0.5055850377484709),\n",
              "  np.float64(0.5027887169509554),\n",
              "  np.float64(0.5000763290513633),\n",
              "  np.float64(0.49745351113304953),\n",
              "  np.float64(0.49489921541606663),\n",
              "  np.float64(0.49241826711316944),\n",
              "  np.float64(0.4900245564028997),\n",
              "  np.float64(0.4876827968073847),\n",
              "  np.float64(0.48538001694668015),\n",
              "  np.float64(0.48314577489774607),\n",
              "  np.float64(0.4809561044602104),\n",
              "  np.float64(0.4788270036982167),\n",
              "  np.float64(0.4767531818306232),\n",
              "  np.float64(0.4747433870188198),\n",
              "  np.float64(0.47280089580490214),\n",
              "  np.float64(0.4709113577812376),\n",
              "  np.float64(0.4690700807707208),\n",
              "  np.float64(0.467269187669952),\n",
              "  np.float64(0.46550770997787383),\n",
              "  np.float64(0.4638007862536809),\n",
              "  np.float64(0.462134490010601),\n",
              "  np.float64(0.4605086888915136),\n",
              "  np.float64(0.45892114393617284),\n",
              "  np.float64(0.4573738277563305),\n",
              "  np.float64(0.45586484254810206),\n",
              "  np.float64(0.45439917618615266),\n",
              "  np.float64(0.4529633748415962),\n",
              "  np.float64(0.4515557581067918),\n",
              "  np.float64(0.45017824236814336),\n",
              "  np.float64(0.44881774185431306),\n",
              "  np.float64(0.44747850332624034),\n",
              "  np.float64(0.44615949070804917),\n",
              "  np.float64(0.44485573882382223),\n",
              "  np.float64(0.44359308730800023),\n",
              "  np.float64(0.44236429871226507),\n",
              "  np.float64(0.44116145047936406),\n",
              "  np.float64(0.43997990946864474),\n",
              "  np.float64(0.4388236215060582),\n",
              "  np.float64(0.43768483509630884),\n",
              "  np.float64(0.43656805452285136),\n",
              "  np.float64(0.43547313633621537),\n",
              "  np.float64(0.43439790479598006),\n",
              "  np.float64(0.4333401851976634),\n",
              "  np.float64(0.43229635274203154),\n",
              "  np.float64(0.43126763675161023),\n",
              "  np.float64(0.4302566291764293),\n",
              "  np.float64(0.42926025609605495),\n",
              "  np.float64(0.4282675580551147),\n",
              "  np.float64(0.4272933697577255),\n",
              "  np.float64(0.4263290445691523),\n",
              "  np.float64(0.4253700717131333),\n",
              "  np.float64(0.4244322579301098),\n",
              "  np.float64(0.4235102380945115),\n",
              "  np.float64(0.4226004471039867),\n",
              "  np.float64(0.42170852226810407),\n",
              "  np.float64(0.4208264748682516),\n",
              "  np.float64(0.4199539448962337),\n",
              "  np.float64(0.4190964321788464),\n",
              "  np.float64(0.418249119642541),\n",
              "  np.float64(0.41741228105613337),\n",
              "  np.float64(0.4165878218026138),\n",
              "  np.float64(0.4157750868067007),\n",
              "  np.float64(0.4149754930715474),\n",
              "  np.float64(0.4141861684769388),\n",
              "  np.float64(0.4134078346667787),\n",
              "  np.float64(0.41263747410081036),\n",
              "  np.float64(0.41187740550595736),\n",
              "  np.float64(0.4111248963475525),\n",
              "  np.float64(0.41037860314905106),\n",
              "  np.float64(0.40963992924280934),\n",
              "  np.float64(0.40891162310160684),\n",
              "  np.float64(0.40819456115981806),\n",
              "  np.float64(0.4074852986405771),\n",
              "  np.float64(0.40678149437499833),\n",
              "  np.float64(0.4060781589787299),\n",
              "  np.float64(0.4053839643193112),\n",
              "  np.float64(0.4046964079971937),\n",
              "  np.float64(0.4040196164318045),\n",
              "  np.float64(0.4033475887696546),\n",
              "  np.float64(0.40268130144051884),\n",
              "  np.float64(0.4020208585949778),\n",
              "  np.float64(0.4013635535806957),\n",
              "  np.float64(0.4007122863484799),\n",
              "  np.float64(0.4000664439917062),\n",
              "  np.float64(0.39942754282452764),\n",
              "  np.float64(0.39879318793861995),\n",
              "  np.float64(0.39816472028026945),\n",
              "  np.float64(0.39754112324944746),\n",
              "  np.float64(0.3969228284529979),\n",
              "  np.float64(0.3963105445383308),\n",
              "  np.float64(0.3957033848458446),\n",
              "  np.float64(0.3951023507884723),\n",
              "  np.float64(0.39450494959183824),\n",
              "  np.float64(0.39390973788511763),\n",
              "  np.float64(0.393321043289297),\n",
              "  np.float64(0.3927363332154511),\n",
              "  np.float64(0.3921566350402797),\n",
              "  np.float64(0.3915800896403142),\n",
              "  np.float64(0.3910103208524673),\n",
              "  np.float64(0.39044596375268353),\n",
              "  np.float64(0.3898879640963783),\n",
              "  np.float64(0.3893315613183254),\n",
              "  np.float64(0.3887802612191756),\n",
              "  np.float64(0.38823227171966906),\n",
              "  np.float64(0.38768846293086673),\n",
              "  np.float64(0.38714501181058314),\n",
              "  np.float64(0.386601180708856),\n",
              "  np.float64(0.38606110549767586),\n",
              "  np.float64(0.38552339498135213),\n",
              "  np.float64(0.3849896550252087),\n",
              "  np.float64(0.38445701706442187),\n",
              "  np.float64(0.3839283891440035),\n",
              "  np.float64(0.3834063912988826),\n",
              "  np.float64(0.3828877895419103),\n",
              "  np.float64(0.3823742868617873),\n",
              "  np.float64(0.3818617762744074),\n",
              "  np.float64(0.38135284037806133),\n",
              "  np.float64(0.38085373176098314),\n",
              "  np.float64(0.3803571487612094),\n",
              "  np.float64(0.3798635286284307),\n",
              "  np.float64(0.37937344659117883),\n",
              "  np.float64(0.37888415142311027),\n",
              "  np.float64(0.37839721026776085),\n",
              "  np.float64(0.37791280657351434),\n",
              "  np.float64(0.37743064703164425),\n",
              "  np.float64(0.37695118684008105),\n",
              "  np.float64(0.3764755535296351),\n",
              "  np.float64(0.37600450918380346),\n",
              "  np.float64(0.3755370282045453),\n",
              "  np.float64(0.37507233482868596),\n",
              "  np.float64(0.37461075404492056),\n",
              "  np.float64(0.3741531831125979),\n",
              "  np.float64(0.37369613526760365),\n",
              "  np.float64(0.3732406665790415),\n",
              "  np.float64(0.3727872844349054),\n",
              "  np.float64(0.37233657618391675),\n",
              "  np.float64(0.37188733352984804),\n",
              "  np.float64(0.37143990163334417),\n",
              "  np.float64(0.3709955322900663),\n",
              "  np.float64(0.37055381521180636),\n",
              "  np.float64(0.3701164248419357),\n",
              "  np.float64(0.3696837980870325),\n",
              "  np.float64(0.36925421414387966),\n",
              "  np.float64(0.3688260165325355),\n",
              "  np.float64(0.3683995803742824),\n",
              "  np.float64(0.36797565011712746),\n",
              "  np.float64(0.36755322597080176),\n",
              "  np.float64(0.3671326414745695),\n",
              "  np.float64(0.36671440652647597),\n",
              "  np.float64(0.36629755186868324),\n",
              "  np.float64(0.365882539942813),\n",
              "  np.float64(0.36546992643810106),\n",
              "  np.float64(0.36505806159681164),\n",
              "  np.float64(0.36464930805658496),\n",
              "  np.float64(0.3642403354170795),\n",
              "  np.float64(0.36383300459690926),\n",
              "  np.float64(0.36342656457873573),\n",
              "  np.float64(0.3630211245273147),\n",
              "  np.float64(0.36261766745320134),\n",
              "  np.float64(0.36221551684479864),\n",
              "  np.float64(0.36181393149885543),\n",
              "  np.float64(0.3614136853548949),\n",
              "  np.float64(0.3610149930691289),\n",
              "  np.float64(0.36061830958142427),\n",
              "  np.float64(0.3602238616094556),\n",
              "  np.float64(0.3598308149937909),\n",
              "  np.float64(0.3594405973531206),\n",
              "  np.float64(0.3590523333645216),\n",
              "  np.float64(0.3586646753681823),\n",
              "  np.float64(0.35827829117989374),\n",
              "  np.float64(0.3578933836800329),\n",
              "  np.float64(0.3575098397745041),\n",
              "  np.float64(0.35712717700164387),\n",
              "  np.float64(0.35674640354927106),\n",
              "  np.float64(0.3563654097619905),\n",
              "  np.float64(0.35598563147751394),\n",
              "  np.float64(0.35560665514905015),\n",
              "  np.float64(0.35522889775467825),\n",
              "  np.float64(0.3548524355957298),\n",
              "  np.float64(0.3544768545761241),\n",
              "  np.float64(0.35410220804562476),\n",
              "  np.float64(0.35372935060809707),\n",
              "  np.float64(0.35335747112747923),\n",
              "  np.float64(0.35298653762865473),\n",
              "  np.float64(0.35261548996406206),\n",
              "  np.float64(0.35224545534722895),\n",
              "  np.float64(0.35187650403326115),\n",
              "  np.float64(0.3515091752109535),\n",
              "  np.float64(0.351142562973503),\n",
              "  np.float64(0.3507761988917742),\n",
              "  np.float64(0.3504121272994944),\n",
              "  np.float64(0.3500508323456739),\n",
              "  np.float64(0.3496902656119949),\n",
              "  np.float64(0.34933072597384834),\n",
              "  np.float64(0.34897156577229405),\n",
              "  np.float64(0.3486130132082588),\n",
              "  np.float64(0.34825543406729104),\n",
              "  np.float64(0.3478987262599147),\n",
              "  np.float64(0.3475425006041821),\n",
              "  np.float64(0.34718754908636146),\n",
              "  np.float64(0.34683374476693674),\n",
              "  np.float64(0.34648068964375317),\n",
              "  np.float64(0.3461286914275414),\n",
              "  np.float64(0.34577770970850386),\n",
              "  np.float64(0.3454277610438636),\n",
              "  np.float64(0.34507890321237183),\n",
              "  np.float64(0.34472958952521904),\n",
              "  np.float64(0.34438123036385293),\n",
              "  np.float64(0.3440338318849381),\n",
              "  np.float64(0.34368682480798224),\n",
              "  np.float64(0.3433406729695961),\n",
              "  np.float64(0.34299504811978426),\n",
              "  np.float64(0.3426507951767031),\n",
              "  np.float64(0.3423068025737934),\n",
              "  np.float64(0.3419641345778042),\n",
              "  np.float64(0.34162206301219467),\n",
              "  np.float64(0.3412804194361408),\n",
              "  np.float64(0.3409397652461298),\n",
              "  np.float64(0.34059967157833376),\n",
              "  np.float64(0.3402611433255801),\n",
              "  np.float64(0.3399241835385416),\n",
              "  np.float64(0.33958889103643963),\n",
              "  np.float64(0.33925367839600745),\n",
              "  np.float64(0.33892007752071635),\n",
              "  np.float64(0.33858715109014786),\n",
              "  np.float64(0.33825564826323046),\n",
              "  np.float64(0.3379240206412329),\n",
              "  np.float64(0.33759326410765683),\n",
              "  np.float64(0.3372623396625386),\n",
              "  np.float64(0.33693286519143867),\n",
              "  np.float64(0.33660412849994853),\n",
              "  np.float64(0.3362764669957539),\n",
              "  np.float64(0.3359483748408573),\n",
              "  np.float64(0.33562110185794897),\n",
              "  np.float64(0.33529418452715376),\n",
              "  np.float64(0.334967911523271),\n",
              "  np.float64(0.3346423098795493),\n",
              "  np.float64(0.334316840034994),\n",
              "  np.float64(0.33399259265634346),\n",
              "  np.float64(0.33366862226911115),\n",
              "  np.float64(0.333345598908006),\n",
              "  np.float64(0.33302295271912796),\n",
              "  np.float64(0.33270008273460044),\n",
              "  np.float64(0.3323777886284562),\n",
              "  np.float64(0.332056918476172),\n",
              "  np.float64(0.331736625715097),\n",
              "  np.float64(0.3314171089125593),\n",
              "  np.float64(0.3310987952578886),\n",
              "  np.float64(0.3307806498231838),\n",
              "  np.float64(0.33046393176764344),\n",
              "  np.float64(0.3301481068394866),\n",
              "  np.float64(0.3298333364419916),\n",
              "  np.float64(0.3295194267864684),\n",
              "  np.float64(0.32920521238996964),\n",
              "  np.float64(0.328892157207032),\n",
              "  np.float64(0.3285803318066772),\n",
              "  np.float64(0.3282680972976897),\n",
              "  np.float64(0.3279576035423614),\n",
              "  np.float64(0.3276471187864756),\n",
              "  np.float64(0.32733738015921787),\n",
              "  np.float64(0.32702780489187017),\n",
              "  np.float64(0.32671907929069866),\n",
              "  np.float64(0.326411292383958),\n",
              "  np.float64(0.32610361330332516),\n",
              "  np.float64(0.3257967559646607),\n",
              "  np.float64(0.3254914256496844),\n",
              "  np.float64(0.32518639541968286),\n",
              "  np.float64(0.32488185284814436),\n",
              "  np.float64(0.32457857428292414),\n",
              "  np.float64(0.3242748911457554),\n",
              "  np.float64(0.32397224877480574),\n",
              "  np.float64(0.32366939114287463),\n",
              "  np.float64(0.3233674489844632),\n",
              "  np.float64(0.32306569877475977),\n",
              "  np.float64(0.3227651667752004),\n",
              "  np.float64(0.32246558252139707),\n",
              "  np.float64(0.32216590906808823),\n",
              "  np.float64(0.3218668846141949),\n",
              "  np.float64(0.32156864359935183),\n",
              "  np.float64(0.3212707210888702),\n",
              "  np.float64(0.32097303349802414),\n",
              "  np.float64(0.3206760268797452),\n",
              "  np.float64(0.32037991759386),\n",
              "  np.float64(0.3200846520291089),\n",
              "  np.float64(0.31979070648454794),\n",
              "  np.float64(0.31949688860185066),\n",
              "  np.float64(0.3192040368830871),\n",
              "  np.float64(0.31891225683920543),\n",
              "  np.float64(0.3186201352661264),\n",
              "  np.float64(0.31832880910106764),\n",
              "  np.float64(0.3180379591780572),\n",
              "  np.float64(0.3177474077512003),\n",
              "  np.float64(0.3174573685881526),\n",
              "  np.float64(0.31716745441576966),\n",
              "  np.float64(0.3168779233122469),\n",
              "  np.float64(0.3165880659464842),\n",
              "  np.float64(0.3162990167273144),\n",
              "  np.float64(0.3160118855471388),\n",
              "  np.float64(0.31572526039332105),\n",
              "  np.float64(0.31543898455468006),\n",
              "  np.float64(0.31515450031384107),\n",
              "  np.float64(0.31486932492833714),\n",
              "  np.float64(0.31458537807423165),\n",
              "  np.float64(0.3143023935190197),\n",
              "  np.float64(0.314020032741554),\n",
              "  np.float64(0.31373820979677214),\n",
              "  np.float64(0.3134564220307837),\n",
              "  np.float64(0.31317604256607157),\n",
              "  np.float64(0.3128958850399918),\n",
              "  np.float64(0.3126166428102297),\n",
              "  np.float64(0.31233763735528963),\n",
              "  np.float64(0.3120598785382099),\n",
              "  np.float64(0.31178223421379686),\n",
              "  np.float64(0.3115016988877591),\n",
              "  np.float64(0.31122327165836333),\n",
              "  np.float64(0.31094437556197246),\n",
              "  np.float64(0.3106662708866108),\n",
              "  np.float64(0.31038756623563823),\n",
              "  np.float64(0.3101076929444198),\n",
              "  np.float64(0.3098280373351554),\n",
              "  np.float64(0.3095488658468458),\n",
              "  np.float64(0.3092708410474206),\n",
              "  np.float64(0.3089925284235495),\n",
              "  np.float64(0.30871521516970696),\n",
              "  np.float64(0.3084382246571843),\n",
              "  np.float64(0.3081608115233644),\n",
              "  np.float64(0.30788505230228885),\n",
              "  np.float64(0.30760889923485535),\n",
              "  np.float64(0.30733370151445155),\n",
              "  np.float64(0.3070587627171204),\n",
              "  np.float64(0.30678549153453466),\n",
              "  np.float64(0.306513402902817),\n",
              "  np.float64(0.30624177204798947),\n",
              "  np.float64(0.30596972267079514),\n",
              "  np.float64(0.3056983773969359),\n",
              "  np.float64(0.30542604453826605),\n",
              "  np.float64(0.30515403041898187),\n",
              "  np.float64(0.3048822657784089),\n",
              "  np.float64(0.3046111388658982),\n",
              "  np.float64(0.30434135862174205),\n",
              "  np.float64(0.3040709677113618),\n",
              "  np.float64(0.303802453935646),\n",
              "  np.float64(0.3035338317347637),\n",
              "  np.float64(0.30326552844890015),\n",
              "  np.float64(0.3029983482206007),\n",
              "  np.float64(0.3027303572284125),\n",
              "  np.float64(0.3024630150156554),\n",
              "  np.float64(0.3021903021328488),\n",
              "  np.float64(0.3019181939869571),\n",
              "  np.float64(0.30164687056194756),\n",
              "  np.float64(0.3013763122138738),\n",
              "  np.float64(0.3011062535351358),\n",
              "  np.float64(0.3008373855054424),\n",
              "  np.float64(0.30056880293040694),\n",
              "  np.float64(0.3002994274233384),\n",
              "  np.float64(0.30003231474402486),\n",
              "  np.float64(0.2997642264500338),\n",
              "  np.float64(0.29949762518388623),\n",
              "  np.float64(0.2992313442583986),\n",
              "  np.float64(0.2989647512830987),\n",
              "  np.float64(0.2987000307000405),\n",
              "  np.float64(0.29843641910151536),\n",
              "  np.float64(0.29817380375484537),\n",
              "  np.float64(0.297910378991521),\n",
              "  np.float64(0.2976476329448143),\n",
              "  np.float64(0.2973858733925377),\n",
              "  np.float64(0.2971234512178799),\n",
              "  np.float64(0.29686185465370674),\n",
              "  np.float64(0.2965993662672583),\n",
              "  np.float64(0.2963389327984143),\n",
              "  np.float64(0.29607895399362893),\n",
              "  np.float64(0.2958187577955834),\n",
              "  np.float64(0.2955605662745591),\n",
              "  np.float64(0.29530208916420536),\n",
              "  np.float64(0.295043141447571),\n",
              "  np.float64(0.2947808434039431),\n",
              "  np.float64(0.2945200193734036),\n",
              "  np.float64(0.29425819804795694),\n",
              "  np.float64(0.2939982138819894),\n",
              "  np.float64(0.29373753863510543),\n",
              "  np.float64(0.2934770759298077),\n",
              "  np.float64(0.2932181769448667),\n",
              "  np.float64(0.2929571020855329),\n",
              "  np.float64(0.2926974273310576),\n",
              "  np.float64(0.292438665425883),\n",
              "  np.float64(0.2921799702994708),\n",
              "  np.float64(0.2919225736533809),\n",
              "  np.float64(0.2916651427011981),\n",
              "  np.float64(0.2914062310039503),\n",
              "  np.float64(0.29114976576529394),\n",
              "  np.float64(0.2908920845402565),\n",
              "  np.float64(0.290636787790647),\n",
              "  np.float64(0.2903815610563729),\n",
              "  np.float64(0.29012611043034786),\n",
              "  np.float64(0.289871850012427),\n",
              "  np.float64(0.28961693516078435),\n",
              "  np.float64(0.2893618213946423),\n",
              "  np.float64(0.2891068515737417),\n",
              "  np.float64(0.28885200934813887),\n",
              "  np.float64(0.288598497335252),\n",
              "  np.float64(0.28834519093203337),\n",
              "  np.float64(0.2880919363510484),\n",
              "  np.float64(0.2878398861253772),\n",
              "  np.float64(0.2875871967554795),\n",
              "  np.float64(0.28733533299236436),\n",
              "  np.float64(0.287084660168824),\n",
              "  np.float64(0.28683401989573976),\n",
              "  np.float64(0.28658445529011617),\n",
              "  np.float64(0.2863351417641881),\n",
              "  np.float64(0.28608598962408344),\n",
              "  np.float64(0.28583881978932274),\n",
              "  np.float64(0.28559101434213824),\n",
              "  np.float64(0.2853441502943621),\n",
              "  np.float64(0.28509794418757217),\n",
              "  np.float64(0.2848519536223844),\n",
              "  np.float64(0.28460657793561467),\n",
              "  np.float64(0.28436114933225604),\n",
              "  np.float64(0.2841171290795743),\n",
              "  np.float64(0.283872650248895),\n",
              "  np.float64(0.2836302422671863),\n",
              "  np.float64(0.28338856551042857),\n",
              "  np.float64(0.2831475744622263),\n",
              "  np.float64(0.2829072496422969),\n",
              "  np.float64(0.28266760600882185),\n",
              "  np.float64(0.2824280646195088),\n",
              "  np.float64(0.2821885917742707),\n",
              "  np.float64(0.2819495066591073),\n",
              "  np.float64(0.2817115899471977),\n",
              "  np.float64(0.28147320328344433),\n",
              "  np.float64(0.28123520538125035),\n",
              "  np.float64(0.2809973814549578),\n",
              "  np.float64(0.2807607091080838),\n",
              "  np.float64(0.2805239245645175),\n",
              "  np.float64(0.2802878320647075),\n",
              "  np.float64(0.28005141633980074),\n",
              "  np.float64(0.2798145923782504),\n",
              "  np.float64(0.27957900121184914),\n",
              "  np.float64(0.27934437271903795),\n",
              "  np.float64(0.27910866380187166),\n",
              "  np.float64(0.27887488778190184),\n",
              "  np.float64(0.2786424193073091),\n",
              "  np.float64(0.27841043074420707),\n",
              "  np.float64(0.2781789933917642),\n",
              "  np.float64(0.27794664273633196),\n",
              "  np.float64(0.277715183625249),\n",
              "  np.float64(0.2774815218908076),\n",
              "  np.float64(0.2772494469091733),\n",
              "  np.float64(0.2770170218361449),\n",
              "  np.float64(0.27678511413547724),\n",
              "  np.float64(0.27655400831660415),\n",
              "  np.float64(0.2763226680409047),\n",
              "  np.float64(0.27609260932738516),\n",
              "  np.float64(0.27586319691909017),\n",
              "  np.float64(0.27563302737513035),\n",
              "  np.float64(0.2754038630077609),\n",
              "  np.float64(0.2751745944171448),\n",
              "  np.float64(0.2749456027857394),\n",
              "  np.float64(0.2747183497615123),\n",
              "  np.float64(0.2744887888956167),\n",
              "  np.float64(0.2742614167338698),\n",
              "  np.float64(0.2740553560091099),\n",
              "  np.float64(0.27385001161416334),\n",
              "  np.float64(0.27364511478810993),\n",
              "  np.float64(0.27344166005899806),\n",
              "  np.float64(0.2732382459149422),\n",
              "  np.float64(0.273033451697395),\n",
              "  np.float64(0.2728312651926046),\n",
              "  np.float64(0.27262809999894155),\n",
              "  np.float64(0.2724229814392788),\n",
              "  np.float64(0.27221795566586304),\n",
              "  np.float64(0.272013489831656),\n",
              "  np.float64(0.27180851027225816),\n",
              "  np.float64(0.2716038523185748),\n",
              "  np.float64(0.27140118957281545),\n",
              "  np.float64(0.2711979623385361),\n",
              "  np.float64(0.27099467668680505),\n",
              "  np.float64(0.27079209152687483),\n",
              "  np.float64(0.2705899020731584),\n",
              "  np.float64(0.2703877200586892),\n",
              "  np.float64(0.27018614538129904),\n",
              "  np.float64(0.269983844738392),\n",
              "  np.float64(0.2697823754298118),\n",
              "  np.float64(0.26958183093583304),\n",
              "  np.float64(0.2693829987392381),\n",
              "  np.float64(0.2691829706465156),\n",
              "  np.float64(0.2689853424424783),\n",
              "  np.float64(0.26878529332089757),\n",
              "  np.float64(0.26858590358761386),\n",
              "  np.float64(0.26838683590541085),\n",
              "  np.float64(0.268188335108164),\n",
              "  np.float64(0.26798936175650545),\n",
              "  np.float64(0.2677915077057274),\n",
              "  np.float64(0.2675933493305307),\n",
              "  np.float64(0.2673963435466532),\n",
              "  np.float64(0.2671982053503152),\n",
              "  np.float64(0.2670004047834736),\n",
              "  np.float64(0.2668029319379459),\n",
              "  np.float64(0.26660455113820536),\n",
              "  np.float64(0.26640725565779866),\n",
              "  np.float64(0.26621068826286515),\n",
              "  np.float64(0.26601239951043665),\n",
              "  np.float64(0.2658154740765242),\n",
              "  np.float64(0.2656177891066426),\n",
              "  np.float64(0.2654204326496832),\n",
              "  np.float64(0.26522358384039757),\n",
              "  np.float64(0.2650248185603221),\n",
              "  np.float64(0.2648271528041456),\n",
              "  np.float64(0.26462976985927184),\n",
              "  np.float64(0.2644318666965813),\n",
              "  np.float64(0.2642329395286633),\n",
              "  np.float64(0.26403443896262524),\n",
              "  np.float64(0.2638362405768976),\n",
              "  np.float64(0.2636396572018719),\n",
              "  np.float64(0.26344215979575913),\n",
              "  np.float64(0.26324484598360925),\n",
              "  np.float64(0.2630485823006091),\n",
              "  np.float64(0.26285236441359133),\n",
              "  np.float64(0.2626559182474233),\n",
              "  np.float64(0.2624603684593709),\n",
              "  np.float64(0.26226425643147155),\n",
              "  np.float64(0.2620678743969628),\n",
              "  np.float64(0.26187225737220604),\n",
              "  np.float64(0.2616771881360709),\n",
              "  np.float64(0.2614814245391921),\n",
              "  np.float64(0.2612873632654334),\n",
              "  np.float64(0.2610932488159925),\n",
              "  np.float64(0.26089901229661944),\n",
              "  np.float64(0.2607049122087022),\n",
              "  np.float64(0.26051082324325264),\n",
              "  np.float64(0.2603176188175049),\n",
              "  np.float64(0.26012453955696213),\n",
              "  np.float64(0.2599317555273069),\n",
              "  np.float64(0.25973966919080715),\n",
              "  np.float64(0.2595471031613468),\n",
              "  np.float64(0.25935521506885395),\n",
              "  np.float64(0.25916365435413735),\n",
              "  np.float64(0.25897245556321963),\n",
              "  np.float64(0.2587818267764171),\n",
              "  np.float64(0.2585915462830759),\n",
              "  np.float64(0.25840016780820696),\n",
              "  np.float64(0.25821166324474637),\n",
              "  np.float64(0.258022075981555),\n",
              "  np.float64(0.257832960593138),\n",
              "  np.float64(0.2576450671690971),\n",
              "  np.float64(0.25745708491283825),\n",
              "  np.float64(0.25726900274203995),\n",
              "  np.float64(0.25708214143971),\n",
              "  np.float64(0.2568981464984408),\n",
              "  np.float64(0.25671219865726047),\n",
              "  np.float64(0.2565278678530237),\n",
              "  np.float64(0.2563432751265831),\n",
              "  np.float64(0.2561581630832703),\n",
              "  np.float64(0.2559732114637721),\n",
              "  np.float64(0.2557892732808557),\n",
              "  np.float64(0.2556053643955756),\n",
              "  np.float64(0.25542127993491437),\n",
              "  np.float64(0.2552388264214694),\n",
              "  np.float64(0.2550558441974887),\n",
              "  np.float64(0.25487412954118027),\n",
              "  np.float64(0.25469165187396986),\n",
              "  np.float64(0.25450922797363584),\n",
              "  np.float64(0.2543277216772518),\n",
              "  np.float64(0.2541473943192295),\n",
              "  np.float64(0.2539652595381619),\n",
              "  np.float64(0.25378383598672377),\n",
              "  np.float64(0.25360240140825624),\n",
              "  np.float64(0.25342251254064646),\n",
              "  np.float64(0.2532420729283746),\n",
              "  np.float64(0.2530616949252812),\n",
              "  np.float64(0.25288158357494916),\n",
              "  np.float64(0.2527018032044712),\n",
              "  np.float64(0.2525227028553095),\n",
              "  np.float64(0.25234287977334124),\n",
              "  np.float64(0.2521646411940119),\n",
              "  np.float64(0.25198581462366443),\n",
              "  np.float64(0.251808489930251),\n",
              "  np.float64(0.25163064773918026),\n",
              "  np.float64(0.2514527338996397),\n",
              "  np.float64(0.25127540637110257),\n",
              "  np.float64(0.2510979322405189),\n",
              "  np.float64(0.2509220507058339),\n",
              "  np.float64(0.2507439623768279),\n",
              "  np.float64(0.25056805673324367),\n",
              "  np.float64(0.2503911374847997),\n",
              "  np.float64(0.25021545225292063),\n",
              "  np.float64(0.2500387610426428),\n",
              "  np.float64(0.24986293897900147),\n",
              "  np.float64(0.24968699182975876),\n",
              "  np.float64(0.24951217674544984),\n",
              "  np.float64(0.2493369127500684),\n",
              "  np.float64(0.24916165536574822),\n",
              "  np.float64(0.24898618198555617),\n",
              "  np.float64(0.2488113732905501),\n",
              "  np.float64(0.24863664053449452),\n",
              "  np.float64(0.24846162640718605),\n",
              "  np.float64(0.2482874908943602),\n",
              "  np.float64(0.24811260313398525),\n",
              "  np.float64(0.2479380000862021),\n",
              "  np.float64(0.24776366015364462),\n",
              "  np.float64(0.24758950177274608),\n",
              "  np.float64(0.24741557317524285),\n",
              "  np.float64(0.24724156019007876),\n",
              "  np.float64(0.24706810579672872),\n",
              "  np.float64(0.2468946812605206),\n",
              "  np.float64(0.2467221544478798),\n",
              "  np.float64(0.24654833676711901),\n",
              "  np.float64(0.24637614726174847),\n",
              "  np.float64(0.24620322800467406),\n",
              "  np.float64(0.24603188650102029),\n",
              "  np.float64(0.24585933543620003),\n",
              "  np.float64(0.24568818931527237),\n",
              "  np.float64(0.2455168853899568),\n",
              "  np.float64(0.2453448745866062),\n",
              "  np.float64(0.24517377264180035),\n",
              "  np.float64(0.2450016473931741),\n",
              "  np.float64(0.24483177307964107),\n",
              "  np.float64(0.24465943628970635),\n",
              "  np.float64(0.24448817067338666),\n",
              "  np.float64(0.24431721877972534),\n",
              "  np.float64(0.24414882094665907),\n",
              "  np.float64(0.2439761262284536),\n",
              "  np.float64(0.24380534226851383),\n",
              "  np.float64(0.24363533914324959),\n",
              "  np.float64(0.24346445276956144),\n",
              "  np.float64(0.24329517412223567),\n",
              "  np.float64(0.2431264276504103),\n",
              "  np.float64(0.24295545352897327),\n",
              "  np.float64(0.24278638110592876),\n",
              "  np.float64(0.24261749767525928),\n",
              "  np.float64(0.24244933575298494),\n",
              "  np.float64(0.2422788360428588),\n",
              "  np.float64(0.2421092278753849),\n",
              "  np.float64(0.2419411239927413),\n",
              "  np.float64(0.24177093685543577),\n",
              "  np.float64(0.24160183684574202),\n",
              "  np.float64(0.24143260101987354),\n",
              "  np.float64(0.24126358845350643),\n",
              "  np.float64(0.2410952487513495),\n",
              "  np.float64(0.2409282709683626),\n",
              "  np.float64(0.24076007661221305),\n",
              "  np.float64(0.2405922095388986),\n",
              "  np.float64(0.24042450942990798),\n",
              "  np.float64(0.24025686354484363),\n",
              "  np.float64(0.2400896337020094),\n",
              "  np.float64(0.23992135622290636),\n",
              "  np.float64(0.23975434374941773),\n",
              "  np.float64(0.23958769955624726),\n",
              "  np.float64(0.23942009529417987),\n",
              "  np.float64(0.23925446384906926),\n",
              "  np.float64(0.2390870288744237),\n",
              "  np.float64(0.23892107536526047),\n",
              "  np.float64(0.23875460594515638),\n",
              "  np.float64(0.23859333460628435),\n",
              "  np.float64(0.23842977796503026),\n",
              "  np.float64(0.23826529786459885),\n",
              "  np.float64(0.23810510252305095),\n",
              "  np.float64(0.23794251838177194),\n",
              "  np.float64(0.23778016708343624),\n",
              "  np.float64(0.23762086183139255),\n",
              "  np.float64(0.23746003580944933),\n",
              "  np.float64(0.23729892668242583),\n",
              "  np.float64(0.23713896586043134),\n",
              "  np.float64(0.23698061661486017),\n",
              "  np.float64(0.23681945252926875),\n",
              "  np.float64(0.23666231415231548),\n",
              "  np.float64(0.23650362926142482),\n",
              "  np.float64(0.2363449020577723),\n",
              "  np.float64(0.23618745606126904),\n",
              "  np.float64(0.2360305011404093),\n",
              "  np.float64(0.2358724465763318),\n",
              "  np.float64(0.23571434047621498),\n",
              "  np.float64(0.23555707960003666),\n",
              "  np.float64(0.23540239255438716),\n",
              "  np.float64(0.23524296414187365),\n",
              "  np.float64(0.23508547920341827),\n",
              "  np.float64(0.2349275274995287),\n",
              "  np.float64(0.23477212325533464),\n",
              "  np.float64(0.2346143062223339),\n",
              "  np.float64(0.23445815190352126),\n",
              "  np.float64(0.23430196683395338),\n",
              "  np.float64(0.23414695018017986),\n",
              "  np.float64(0.23399084920135912),\n",
              "  np.float64(0.23383468972993726),\n",
              "  np.float64(0.23368238891558865),\n",
              "  np.float64(0.2335286983908585),\n",
              "  np.float64(0.23337436165418046),\n",
              "  np.float64(0.23322176300098724),\n",
              "  np.float64(0.23306808722531747),\n",
              "  np.float64(0.23291545022968355),\n",
              "  np.float64(0.2327621546329321),\n",
              "  np.float64(0.23260941806192192),\n",
              "  np.float64(0.2324563787410842),\n",
              "  np.float64(0.2323047250914265),\n",
              "  np.float64(0.23215170050815886),\n",
              "  np.float64(0.23199881851965853),\n",
              "  np.float64(0.23184710871346487),\n",
              "  np.float64(0.2316954992244801),\n",
              "  np.float64(0.23154239100851332),\n",
              "  np.float64(0.2313921532915996),\n",
              "  np.float64(0.2312410513365997),\n",
              "  np.float64(0.23108940820018609),\n",
              "  np.float64(0.23093580505828123),\n",
              "  np.float64(0.23078586496264225),\n",
              "  np.float64(0.2306330566469909),\n",
              "  np.float64(0.2304819486330823),\n",
              "  np.float64(0.2303328575699776),\n",
              "  np.float64(0.23018120773479953),\n",
              "  np.float64(0.23003072672587707),\n",
              "  np.float64(0.2298830765520383),\n",
              "  np.float64(0.22973269238648855),\n",
              "  np.float64(0.22958236231298912),\n",
              "  np.float64(0.22943370912458919),\n",
              "  np.float64(0.22928369592690187),\n",
              "  np.float64(0.22913521296001732),\n",
              "  np.float64(0.22898392702050405),\n",
              "  np.float64(0.22883590592859498),\n",
              "  np.float64(0.22868609564397294),\n",
              "  np.float64(0.22853707761693345),\n",
              "  np.float64(0.22838890436372933),\n",
              "  np.float64(0.2282410072233743),\n",
              "  np.float64(0.2280914178431091),\n",
              "  np.float64(0.2279435866738807),\n",
              "  np.float64(0.22779556146272836),\n",
              "  np.float64(0.2276466777753805),\n",
              "  np.float64(0.22749844550707948),\n",
              "  np.float64(0.2273512967206503),\n",
              "  np.float64(0.22720338340125418),\n",
              "  np.float64(0.22705526486046093),\n",
              "  np.float64(0.22690817393461865),\n",
              "  np.float64(0.2267620467534151),\n",
              "  np.float64(0.2266136040124604),\n",
              "  np.float64(0.22646606718152487),\n",
              "  np.float64(0.22632000781604503),\n",
              "  np.float64(0.22617409467640276),\n",
              "  np.float64(0.22602814274948985),\n",
              "  np.float64(0.22588139071517332),\n",
              "  np.float64(0.2257373886862599),\n",
              "  np.float64(0.22559225690919713),\n",
              "  np.float64(0.22544619948394798),\n",
              "  np.float64(0.22530440840499086),\n",
              "  np.float64(0.22515992060155637),\n",
              "  np.float64(0.22501678989585228),\n",
              "  np.float64(0.2248730662638451),\n",
              "  np.float64(0.22473009967051474),\n",
              "  np.float64(0.22458713146871073),\n",
              "  np.float64(0.22444324124894374),\n",
              "  np.float64(0.22429912681342745),\n",
              "  np.float64(0.22415851434480008),\n",
              "  np.float64(0.22401456516227458),\n",
              "  np.float64(0.22387161526583507),\n",
              "  np.float64(0.22372877869273372),\n",
              "  np.float64(0.2235880633828736),\n",
              "  np.float64(0.22344342714766577),\n",
              "  np.float64(0.22330046227728756),\n",
              "  np.float64(0.22316105028436145),\n",
              "  np.float64(0.22301664689419856),\n",
              "  np.float64(0.22287497636233666),\n",
              "  np.float64(0.22273210958947012),\n",
              "  np.float64(0.22259149672222464),\n",
              "  np.float64(0.22244838079278692),\n",
              "  np.float64(0.22230826779319418),\n",
              "  np.float64(0.22216446944428297),\n",
              "  np.float64(0.22202524011079589),\n",
              "  np.float64(0.22188401654357576),\n",
              "  np.float64(0.2217423923514923),\n",
              "  np.float64(0.22159941166155409),\n",
              "  np.float64(0.2214590971537116),\n",
              "  np.float64(0.22131666289949864),\n",
              "  np.float64(0.22117654934687156),\n",
              "  np.float64(0.22103560261165833),\n",
              "  np.float64(0.22089644603009093),\n",
              "  np.float64(0.2207546642317935),\n",
              "  np.float64(0.22061281049369383),\n",
              "  np.float64(0.22047301496836327),\n",
              "  np.float64(0.22033395348307236),\n",
              "  np.float64(0.22019292306746907),\n",
              "  np.float64(0.22005307253745254),\n",
              "  np.float64(0.21991295401154398),\n",
              "  np.float64(0.21977420167873746),\n",
              "  np.float64(0.21963334030970708),\n",
              "  np.float64(0.21949350503459575),\n",
              "  np.float64(0.2193554424269533),\n",
              "  np.float64(0.2192158705060127),\n",
              "  np.float64(0.2190779392665762),\n",
              "  np.float64(0.2189387946389458),\n",
              "  np.float64(0.2187986265569165),\n",
              "  np.float64(0.21866060039677115),\n",
              "  np.float64(0.21852360064400653),\n",
              "  np.float64(0.21838440407377266),\n",
              "  np.float64(0.21824539514624514),\n",
              "  np.float64(0.2181072842445533),\n",
              "  np.float64(0.21796988062879305),\n",
              "  np.float64(0.21783279956302193),\n",
              "  np.float64(0.21769361111004853),\n",
              "  np.float64(0.2175555201345591),\n",
              "  np.float64(0.21741907773040026),\n",
              "  np.float64(0.21728246730666967),\n",
              "  np.float64(0.21714394076288424),\n",
              "  np.float64(0.2170065894596264),\n",
              "  np.float64(0.21686948907555015),\n",
              "  np.float64(0.2167355228941275),\n",
              "  np.float64(0.2165971669497765),\n",
              "  np.float64(0.2164610865715256),\n",
              "  np.float64(0.21632511988444922),\n",
              "  np.float64(0.21618821332777347),\n",
              "  np.float64(0.21605339347552793),\n",
              "  np.float64(0.2159175005654321),\n",
              "  np.float64(0.21578064996052135),\n",
              "  np.float64(0.2156463345935182),\n",
              "  np.float64(0.21551170036788475),\n",
              "  np.float64(0.21537668856670344),\n",
              "  np.float64(0.21523968494814014),\n",
              "  np.float64(0.215104967099314),\n",
              "  np.float64(0.21497060400555504),\n",
              "  np.float64(0.21483543447188375),\n",
              "  np.float64(0.21470154937342695),\n",
              "  np.float64(0.21456872739991828),\n",
              "  np.float64(0.21443354280631868),\n",
              "  np.float64(0.2142994646610958),\n",
              "  np.float64(0.21416681064532408),\n",
              "  np.float64(0.2140326055722475),\n",
              "  np.float64(0.21389983856772188),\n",
              "  np.float64(0.21376690158339012),\n",
              "  np.float64(0.2136343638602316),\n",
              "  np.float64(0.2135030788166428),\n",
              "  np.float64(0.21336972995684092),\n",
              "  np.float64(0.2132379977566075),\n",
              "  np.float64(0.21310531745675487),\n",
              "  np.float64(0.21297383528958894),\n",
              "  np.float64(0.21284342049286104),\n",
              "  np.float64(0.21271540955817797),\n",
              "  np.float64(0.21258162647764128),\n",
              "  np.float64(0.21245007673213753),\n",
              "  np.float64(0.21232000732885506),\n",
              "  np.float64(0.21218852766592244),\n",
              "  np.float64(0.21206071701065715),\n",
              "  np.float64(0.21192928710701478),\n",
              "  np.float64(0.2118002541852876),\n",
              "  np.float64(0.21167041563136502),\n",
              "  np.float64(0.211539662000722),\n",
              "  np.float64(0.2114110945799665),\n",
              "  np.float64(0.2112836848295273),\n",
              "  np.float64(0.21115307804340636),\n",
              "  np.float64(0.21102308250820082),\n",
              "  np.float64(0.21089479401145675),\n",
              "  np.float64(0.2107660088267331),\n",
              "  np.float64(0.21063746108895975),\n",
              "  np.float64(0.21050929939252822),\n",
              "  np.float64(0.21038160011607895),\n",
              "  np.float64(0.210254084233751),\n",
              "  np.float64(0.21012467045539746),\n",
              "  np.float64(0.20999814357416124),\n",
              "  np.float64(0.2098700747809184),\n",
              "  np.float64(0.2097424667200501),\n",
              "  np.float64(0.20961459444348415),\n",
              "  np.float64(0.20948636487342595),\n",
              "  np.float64(0.20935796850817218),\n",
              "  np.float64(0.20923152172667087),\n",
              "  np.float64(0.20910384146997388),\n",
              "  np.float64(0.20897542239900674),\n",
              "  np.float64(0.20885088856617545),\n",
              "  np.float64(0.20872149264101864),\n",
              "  np.float64(0.20859478610753815),\n",
              "  np.float64(0.20846731830187956),\n",
              "  np.float64(0.20833920281413723),\n",
              "  np.float64(0.2082146512647401),\n",
              "  np.float64(0.20808624150625155),\n",
              "  np.float64(0.20796152155326383),\n",
              "  np.float64(0.20783748853569814),\n",
              "  np.float64(0.20770877384343198),\n",
              "  np.float64(0.20758386375150284),\n",
              "  np.float64(0.20745626155198835),\n",
              "  np.float64(0.20733303856828722),\n",
              "  np.float64(0.20720725708982404),\n",
              "  np.float64(0.20708175048354405),\n",
              "  np.float64(0.20695611965838892),\n",
              "  np.float64(0.20683341016056678),\n",
              "  np.float64(0.2067069019888743),\n",
              "  np.float64(0.2065824976220486),\n",
              "  np.float64(0.20645789821247973),\n",
              "  np.float64(0.20633539013691396),\n",
              "  np.float64(0.20620965005040018),\n",
              "  np.float64(0.2060848551137363),\n",
              "  np.float64(0.2059594474469866),\n",
              "  np.float64(0.20584000508386657),\n",
              "  np.float64(0.20571228325864982),\n",
              "  np.float64(0.20558836453839163),\n",
              "  np.float64(0.2054655511099955),\n",
              "  np.float64(0.20534164052024917),\n",
              "  np.float64(0.2052177300777902),\n",
              "  np.float64(0.205093408476868),\n",
              "  np.float64(0.20497219670646608),\n",
              "  np.float64(0.20485075844030134),\n",
              "  np.float64(0.20472437278032057),\n",
              "  np.float64(0.20460111377710222),\n",
              "  np.float64(0.2044801777914203),\n",
              "  np.float64(0.20435667576013092),\n",
              "  np.float64(0.20423253103637828),\n",
              "  np.float64(0.2041089913659957),\n",
              "  np.float64(0.20398602130056046),\n",
              "  np.float64(0.2038648260778606),\n",
              "  np.float64(0.20374568694600081),\n",
              "  np.float64(0.20361933036390514),\n",
              "  np.float64(0.20349817826387628),\n",
              "  np.float64(0.20337636895000108),\n",
              "  np.float64(0.20325353970100934),\n",
              "  np.float64(0.20313367154413386),\n",
              "  np.float64(0.20301097156247652),\n",
              "  np.float64(0.20289259329867426),\n",
              "  np.float64(0.2027673911271552),\n",
              "  np.float64(0.20264722897342552),\n",
              "  np.float64(0.20252642418840386),\n",
              "  np.float64(0.20240809568532253),\n",
              "  np.float64(0.20228845464479062),\n",
              "  np.float64(0.20216464034157255),\n",
              "  np.float64(0.2020453948504714),\n",
              "  np.float64(0.20192886661413562),\n",
              "  np.float64(0.20180756337964925),\n",
              "  np.float64(0.2016850990194565),\n",
              "  np.float64(0.20156622835271956),\n",
              "  np.float64(0.20144562118322873),\n",
              "  np.float64(0.20133000490500627),\n",
              "  np.float64(0.2012077446417854),\n",
              "  np.float64(0.2010892186574365),\n",
              "  np.float64(0.20096805690695368),\n",
              "  np.float64(0.20085175335876554),\n",
              "  np.float64(0.20072987686217544),\n",
              "  np.float64(0.20061316226942605),\n",
              "  np.float64(0.20049203299377927),\n",
              "  np.float64(0.20037268190297303),\n",
              "  np.float64(0.20025303244288203),\n",
              "  np.float64(0.20013454230314584),\n",
              "  np.float64(0.20001755903956792),\n",
              "  np.float64(0.19989898423891428),\n",
              "  np.float64(0.19977937868479784),\n",
              "  np.float64(0.19966052505787618),\n",
              "  np.float64(0.199543381315187),\n",
              "  np.float64(0.19942664224604914),\n",
              "  np.float64(0.199305341647266),\n",
              "  np.float64(0.19918661531491155),\n",
              "  np.float64(0.19907399257471936),\n",
              "  np.float64(0.19895341514786916),\n",
              "  np.float64(0.19883384624359146),\n",
              "  np.float64(0.19871809752208822),\n",
              "  np.float64(0.198598645662341),\n",
              "  np.float64(0.19848409459829983),\n",
              "  np.float64(0.19836766541422116),\n",
              "  np.float64(0.1982495701204743),\n",
              "  np.float64(0.19813012317187312),\n",
              "  np.float64(0.19801466475732327),\n",
              "  np.float64(0.1978991061778417),\n",
              "  np.float64(0.197782697514351),\n",
              "  np.float64(0.1976642534926124),\n",
              "  np.float64(0.1975485642900051),\n",
              "  np.float64(0.19742939477315877),\n",
              "  np.float64(0.19731413412617743),\n",
              "  np.float64(0.19720103739556435),\n",
              "  np.float64(0.19708341863671996),\n",
              "  ...],\n",
              " 'val_costs': None,\n",
              " 'learning_rates': [0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.01,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  0.009000000000000001,\n",
              "  ...]}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics, test_predictions = model.evaluate(X_test_processed, y_test)\n",
        "\n",
        "print(f\"\\nTest accuracy: {metrics['accuracy']:.2f}%\")\n",
        "print(f\"Precision: {metrics['precision']:.2f}%\")\n",
        "print(f\"Recall: {metrics['recall']:.2f}%\")\n",
        "print(f\"F1 Score: {metrics['f1_score']:.2f}%\")\n",
        "print(f\"Loss: {metrics['loss']:.4f}\")\n",
        "\n",
        "test_predictions = test_predictions.flatten()\n",
        "\n",
        "print(\"\\nPredictions vs Actual:\")\n",
        "print(\"----------------------\")\n",
        "for i in range(len(y_test)):\n",
        "    print(f\"Example {i + 1}: Predicted {int(test_predictions[i])}, Actual {int(y_test.iloc[i])}\")\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(y_test, test_predictions)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2yi50XeUJKmC",
        "outputId": "0a5c9be7-1e62-453b-a848-7640c470c3f2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test accuracy: 98.05%\n",
            "Precision: 99.01%\n",
            "Recall: 97.09%\n",
            "F1 Score: 98.04%\n",
            "Loss: 0.1058\n",
            "\n",
            "Predictions vs Actual:\n",
            "----------------------\n",
            "Example 1: Predicted 1, Actual 1\n",
            "Example 2: Predicted 1, Actual 1\n",
            "Example 3: Predicted 0, Actual 0\n",
            "Example 4: Predicted 1, Actual 1\n",
            "Example 5: Predicted 0, Actual 0\n",
            "Example 6: Predicted 1, Actual 1\n",
            "Example 7: Predicted 0, Actual 0\n",
            "Example 8: Predicted 0, Actual 0\n",
            "Example 9: Predicted 1, Actual 1\n",
            "Example 10: Predicted 0, Actual 0\n",
            "Example 11: Predicted 1, Actual 1\n",
            "Example 12: Predicted 0, Actual 0\n",
            "Example 13: Predicted 1, Actual 1\n",
            "Example 14: Predicted 1, Actual 1\n",
            "Example 15: Predicted 0, Actual 0\n",
            "Example 16: Predicted 0, Actual 0\n",
            "Example 17: Predicted 0, Actual 0\n",
            "Example 18: Predicted 1, Actual 1\n",
            "Example 19: Predicted 1, Actual 1\n",
            "Example 20: Predicted 0, Actual 0\n",
            "Example 21: Predicted 0, Actual 1\n",
            "Example 22: Predicted 0, Actual 0\n",
            "Example 23: Predicted 0, Actual 0\n",
            "Example 24: Predicted 0, Actual 0\n",
            "Example 25: Predicted 0, Actual 1\n",
            "Example 26: Predicted 1, Actual 1\n",
            "Example 27: Predicted 1, Actual 1\n",
            "Example 28: Predicted 1, Actual 1\n",
            "Example 29: Predicted 0, Actual 0\n",
            "Example 30: Predicted 0, Actual 0\n",
            "Example 31: Predicted 0, Actual 0\n",
            "Example 32: Predicted 1, Actual 1\n",
            "Example 33: Predicted 0, Actual 0\n",
            "Example 34: Predicted 1, Actual 1\n",
            "Example 35: Predicted 1, Actual 1\n",
            "Example 36: Predicted 1, Actual 1\n",
            "Example 37: Predicted 0, Actual 1\n",
            "Example 38: Predicted 1, Actual 1\n",
            "Example 39: Predicted 1, Actual 1\n",
            "Example 40: Predicted 1, Actual 1\n",
            "Example 41: Predicted 0, Actual 0\n",
            "Example 42: Predicted 0, Actual 0\n",
            "Example 43: Predicted 1, Actual 1\n",
            "Example 44: Predicted 0, Actual 0\n",
            "Example 45: Predicted 0, Actual 0\n",
            "Example 46: Predicted 0, Actual 0\n",
            "Example 47: Predicted 0, Actual 0\n",
            "Example 48: Predicted 0, Actual 0\n",
            "Example 49: Predicted 0, Actual 0\n",
            "Example 50: Predicted 1, Actual 1\n",
            "Example 51: Predicted 1, Actual 1\n",
            "Example 52: Predicted 0, Actual 0\n",
            "Example 53: Predicted 0, Actual 0\n",
            "Example 54: Predicted 0, Actual 0\n",
            "Example 55: Predicted 1, Actual 1\n",
            "Example 56: Predicted 1, Actual 1\n",
            "Example 57: Predicted 0, Actual 0\n",
            "Example 58: Predicted 0, Actual 0\n",
            "Example 59: Predicted 0, Actual 0\n",
            "Example 60: Predicted 1, Actual 1\n",
            "Example 61: Predicted 1, Actual 1\n",
            "Example 62: Predicted 1, Actual 1\n",
            "Example 63: Predicted 0, Actual 0\n",
            "Example 64: Predicted 1, Actual 1\n",
            "Example 65: Predicted 0, Actual 0\n",
            "Example 66: Predicted 0, Actual 0\n",
            "Example 67: Predicted 1, Actual 1\n",
            "Example 68: Predicted 0, Actual 0\n",
            "Example 69: Predicted 0, Actual 0\n",
            "Example 70: Predicted 1, Actual 1\n",
            "Example 71: Predicted 0, Actual 0\n",
            "Example 72: Predicted 0, Actual 0\n",
            "Example 73: Predicted 0, Actual 0\n",
            "Example 74: Predicted 1, Actual 1\n",
            "Example 75: Predicted 1, Actual 1\n",
            "Example 76: Predicted 1, Actual 1\n",
            "Example 77: Predicted 0, Actual 0\n",
            "Example 78: Predicted 0, Actual 0\n",
            "Example 79: Predicted 0, Actual 0\n",
            "Example 80: Predicted 1, Actual 1\n",
            "Example 81: Predicted 0, Actual 0\n",
            "Example 82: Predicted 0, Actual 0\n",
            "Example 83: Predicted 0, Actual 0\n",
            "Example 84: Predicted 0, Actual 0\n",
            "Example 85: Predicted 1, Actual 1\n",
            "Example 86: Predicted 0, Actual 0\n",
            "Example 87: Predicted 1, Actual 1\n",
            "Example 88: Predicted 0, Actual 0\n",
            "Example 89: Predicted 0, Actual 0\n",
            "Example 90: Predicted 0, Actual 0\n",
            "Example 91: Predicted 0, Actual 0\n",
            "Example 92: Predicted 0, Actual 0\n",
            "Example 93: Predicted 1, Actual 1\n",
            "Example 94: Predicted 1, Actual 1\n",
            "Example 95: Predicted 1, Actual 1\n",
            "Example 96: Predicted 1, Actual 1\n",
            "Example 97: Predicted 0, Actual 0\n",
            "Example 98: Predicted 0, Actual 0\n",
            "Example 99: Predicted 0, Actual 0\n",
            "Example 100: Predicted 0, Actual 0\n",
            "Example 101: Predicted 1, Actual 1\n",
            "Example 102: Predicted 0, Actual 0\n",
            "Example 103: Predicted 0, Actual 0\n",
            "Example 104: Predicted 1, Actual 1\n",
            "Example 105: Predicted 0, Actual 0\n",
            "Example 106: Predicted 1, Actual 1\n",
            "Example 107: Predicted 0, Actual 0\n",
            "Example 108: Predicted 1, Actual 1\n",
            "Example 109: Predicted 0, Actual 0\n",
            "Example 110: Predicted 1, Actual 1\n",
            "Example 111: Predicted 0, Actual 0\n",
            "Example 112: Predicted 1, Actual 1\n",
            "Example 113: Predicted 1, Actual 1\n",
            "Example 114: Predicted 0, Actual 0\n",
            "Example 115: Predicted 1, Actual 1\n",
            "Example 116: Predicted 1, Actual 1\n",
            "Example 117: Predicted 0, Actual 0\n",
            "Example 118: Predicted 1, Actual 1\n",
            "Example 119: Predicted 1, Actual 1\n",
            "Example 120: Predicted 0, Actual 0\n",
            "Example 121: Predicted 1, Actual 1\n",
            "Example 122: Predicted 1, Actual 1\n",
            "Example 123: Predicted 0, Actual 0\n",
            "Example 124: Predicted 0, Actual 0\n",
            "Example 125: Predicted 1, Actual 1\n",
            "Example 126: Predicted 0, Actual 0\n",
            "Example 127: Predicted 1, Actual 1\n",
            "Example 128: Predicted 0, Actual 0\n",
            "Example 129: Predicted 0, Actual 0\n",
            "Example 130: Predicted 1, Actual 1\n",
            "Example 131: Predicted 1, Actual 1\n",
            "Example 132: Predicted 0, Actual 0\n",
            "Example 133: Predicted 1, Actual 1\n",
            "Example 134: Predicted 1, Actual 1\n",
            "Example 135: Predicted 0, Actual 0\n",
            "Example 136: Predicted 1, Actual 1\n",
            "Example 137: Predicted 0, Actual 0\n",
            "Example 138: Predicted 1, Actual 1\n",
            "Example 139: Predicted 1, Actual 1\n",
            "Example 140: Predicted 0, Actual 0\n",
            "Example 141: Predicted 1, Actual 1\n",
            "Example 142: Predicted 1, Actual 1\n",
            "Example 143: Predicted 1, Actual 1\n",
            "Example 144: Predicted 1, Actual 1\n",
            "Example 145: Predicted 1, Actual 1\n",
            "Example 146: Predicted 1, Actual 1\n",
            "Example 147: Predicted 1, Actual 1\n",
            "Example 148: Predicted 1, Actual 1\n",
            "Example 149: Predicted 0, Actual 0\n",
            "Example 150: Predicted 0, Actual 0\n",
            "Example 151: Predicted 0, Actual 0\n",
            "Example 152: Predicted 0, Actual 0\n",
            "Example 153: Predicted 1, Actual 1\n",
            "Example 154: Predicted 1, Actual 1\n",
            "Example 155: Predicted 0, Actual 0\n",
            "Example 156: Predicted 0, Actual 0\n",
            "Example 157: Predicted 0, Actual 0\n",
            "Example 158: Predicted 1, Actual 1\n",
            "Example 159: Predicted 0, Actual 0\n",
            "Example 160: Predicted 0, Actual 0\n",
            "Example 161: Predicted 1, Actual 1\n",
            "Example 162: Predicted 1, Actual 1\n",
            "Example 163: Predicted 0, Actual 0\n",
            "Example 164: Predicted 0, Actual 0\n",
            "Example 165: Predicted 1, Actual 1\n",
            "Example 166: Predicted 1, Actual 1\n",
            "Example 167: Predicted 0, Actual 0\n",
            "Example 168: Predicted 0, Actual 0\n",
            "Example 169: Predicted 1, Actual 1\n",
            "Example 170: Predicted 1, Actual 1\n",
            "Example 171: Predicted 0, Actual 0\n",
            "Example 172: Predicted 1, Actual 1\n",
            "Example 173: Predicted 1, Actual 1\n",
            "Example 174: Predicted 0, Actual 0\n",
            "Example 175: Predicted 1, Actual 1\n",
            "Example 176: Predicted 1, Actual 1\n",
            "Example 177: Predicted 1, Actual 1\n",
            "Example 178: Predicted 0, Actual 0\n",
            "Example 179: Predicted 0, Actual 0\n",
            "Example 180: Predicted 1, Actual 1\n",
            "Example 181: Predicted 1, Actual 1\n",
            "Example 182: Predicted 0, Actual 0\n",
            "Example 183: Predicted 1, Actual 1\n",
            "Example 184: Predicted 0, Actual 0\n",
            "Example 185: Predicted 1, Actual 1\n",
            "Example 186: Predicted 1, Actual 1\n",
            "Example 187: Predicted 1, Actual 1\n",
            "Example 188: Predicted 0, Actual 0\n",
            "Example 189: Predicted 1, Actual 1\n",
            "Example 190: Predicted 1, Actual 1\n",
            "Example 191: Predicted 1, Actual 1\n",
            "Example 192: Predicted 0, Actual 0\n",
            "Example 193: Predicted 0, Actual 0\n",
            "Example 194: Predicted 0, Actual 0\n",
            "Example 195: Predicted 0, Actual 0\n",
            "Example 196: Predicted 1, Actual 1\n",
            "Example 197: Predicted 1, Actual 0\n",
            "Example 198: Predicted 0, Actual 0\n",
            "Example 199: Predicted 1, Actual 1\n",
            "Example 200: Predicted 1, Actual 1\n",
            "Example 201: Predicted 1, Actual 1\n",
            "Example 202: Predicted 1, Actual 1\n",
            "Example 203: Predicted 1, Actual 1\n",
            "Example 204: Predicted 0, Actual 0\n",
            "Example 205: Predicted 0, Actual 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_model('/content/ModularNeuralNet.npy')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFagKCSldLlb",
        "outputId": "8df86480-91d9-49fe-af3b-39fa055d216e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/ModularNeuralNet.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = ModularNeuralNet.load_model('/content/ModularNeuralNet.npy')\n",
        "\n",
        "predictions = loaded_model.predict(X_test_processed)\n",
        "for i in predictions:\n",
        "  print(i[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCqzjBgNKEJ-",
        "outputId": "fba8bdb1-4022-4ad5-f744-1191d861d67f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "527    1\n",
            "359    1\n",
            "447    0\n",
            "31     1\n",
            "621    0\n",
            "      ..\n",
            "832    1\n",
            "796    1\n",
            "644    1\n",
            "404    0\n",
            "842    0\n",
            "Name: target, Length: 205, dtype: int64\n"
          ]
        }
      ]
    }
  ]
}